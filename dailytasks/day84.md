# ğŸ“˜ Day 84 â€” Applied AI Security

## ğŸ¯ Goals
- Understand attacks on AI & LLMs
- Learn prompt injection, data poisoning, model extraction

## ğŸ“š Learning
OWASP ML/LLM Security Top 10  
https://owasp.org/www-project-top-ten-for-large-language-model-applications/

---

## ğŸ› ï¸ Tasks
Study:
- Prompt injection  
- Data poisoning  
- Insecure outputs  

Write:
ai-security-notes.md

---

# âœï¸ NOTES
-  

# ğŸ§  SELF-TEST QUESTIONS
1. What is prompt injection?  
2. Why are LLMs vulnerable?  
3. What is model extraction?  
4. What is hallucination?  
5. How to protect LLM apps?

# ğŸš€ WHAT I BUILT TODAY
-  

# ğŸ“Œ REFLECTION
-  

# â­ OPTIONAL STRETCH CHALLENGE
- Build a hardened prompt wrapper function.
